{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import hashlib\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import math"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "INPUT_FILE = \"./large_dis_format.csv\"\n",
                "OUTPUT_FILE = \"./k_dp_anonymized_data.csv\"\n",
                "\n",
                "# Secret salt for user ID hashing (never commit real salts to public repos!)\n",
                "SECRET_SALT = \"my_super_secret_salt\"\n",
                "\n",
                "# K-Anonymity parameters\n",
                "K_LOCATION = 10  # Minimum cluster size for location\n",
                "K_TIME = 5       # Minimum bin size for time\n",
                "\n",
                "# Differential Privacy parameters\n",
                "EPSILON_LOCATION = 1.0  # Privacy budget for location\n",
                "EPSILON_TIME = 1.0      # Privacy budget for time\n",
                "\n",
                "# Sensitivity assumptions for Laplace noise\n",
                "#   Latitude in [-90,90] -> total range ~180\n",
                "#   Longitude in [-180,180] -> total range ~360\n",
                "SENSITIVITY_LAT = 180.0\n",
                "SENSITIVITY_LON = 360.0\n",
                "\n",
                "#   Hour in [0,23] -> total range ~24\n",
                "#   Minute in [0,59] -> total range ~60\n",
                "SENSITIVITY_HOUR = 24.0\n",
                "SENSITIVITY_MINUTE = 60.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(filepath=INPUT_FILE):\n",
                "    \"\"\"\n",
                "    Expecting a tab-separated file with columns: id, date, latitude, longitude\n",
                "    (no header row).\n",
                "    \"\"\"\n",
                "    columns_name = ['id', 'date', 'latitude', 'longitude']\n",
                "    df = pd.read_csv(filepath, names=columns_name, sep='\\t')\n",
                "    df['date'] = pd.to_datetime(df['date'])\n",
                "    return df\n",
                "\n",
                "def anonymize_id(df, id_col='id', date_col='date', salt=SECRET_SALT):\n",
                "    \"\"\"\n",
                "    Anonymize user ID by hashing (user_id + date's ISO week + salt).\n",
                "    If you want the same user ID across all weeks, remove `week` from the hash.\n",
                "    \"\"\"\n",
                "    # Extract ISO week\n",
                "    df['week'] = df[date_col].dt.isocalendar().week\n",
                "    \n",
                "    def hash_func(row):\n",
                "        pseudo = f\"{salt}_{row[id_col]}_{row['week']}\"\n",
                "        return hashlib.sha256(pseudo.encode()).hexdigest()[:10]\n",
                "\n",
                "    df['anonymized_id'] = df.apply(hash_func, axis=1)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def strict_k_clusters_location(data, lat_col='latitude', lon_col='longitude', k_location=10):\n",
                "    coords = data[[lat_col, lon_col]].values\n",
                "    scaler = StandardScaler()\n",
                "    scaled_coords = scaler.fit_transform(coords)\n",
                "\n",
                "    # Initial K-Means\n",
                "    n_clusters = max(1, len(data) // k_location)\n",
                "    if n_clusters < 2:\n",
                "        data['location_cluster'] = 0\n",
                "    else:\n",
                "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
                "        data['location_cluster'] = kmeans.fit_predict(scaled_coords)\n",
                "\n",
                "    # Merge small clusters until all have >= k_location\n",
                "    while True:\n",
                "        sizes = data['location_cluster'].value_counts()\n",
                "        small_clusters = sizes[sizes < k_location]\n",
                "        if small_clusters.empty:\n",
                "            break\n",
                "        for cid in small_clusters.index:\n",
                "            target_cid = sizes.idxmax()\n",
                "            data.loc[data['location_cluster'] == cid, 'location_cluster'] = target_cid\n",
                "            sizes = data['location_cluster'].value_counts()\n",
                "            small_clusters = sizes[sizes < k_location]\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dp_laplace_mechanism(value, epsilon, sensitivity):\n",
                "    scale = sensitivity / epsilon\n",
                "    noise = np.random.laplace(loc=0.0, scale=scale)\n",
                "    return value + noise\n",
                "\n",
                "def dp_on_location_clusters(data,\n",
                "                            lat_col='latitude', lon_col='longitude',\n",
                "                            cluster_col='location_cluster',\n",
                "                            epsilon_loc=EPSILON_LOCATION,\n",
                "                            sensitivity_lat=SENSITIVITY_LAT,\n",
                "                            sensitivity_lon=SENSITIVITY_LON):\n",
                "    dp_lookup = {}\n",
                "    for cid in data[cluster_col].unique():\n",
                "        subset = data[data[cluster_col] == cid]\n",
                "        true_lat = subset[lat_col].mean()\n",
                "        true_lon = subset[lon_col].mean()\n",
                "\n",
                "        dp_lat = dp_laplace_mechanism(true_lat, epsilon_loc, sensitivity_lat)\n",
                "        dp_lon = dp_laplace_mechanism(true_lon, epsilon_loc, sensitivity_lon)\n",
                "\n",
                "        dp_lookup[cid] = (dp_lat, dp_lon)\n",
                "\n",
                "    data['anonymized_latitude'] = data[cluster_col].apply(lambda c: dp_lookup[c][0])\n",
                "    data['anonymized_longitude'] = data[cluster_col].apply(lambda c: dp_lookup[c][1])\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def time_binning_k_anonymity(data, date_col='date', k_time=K_TIME):\n",
                "    data['time_bin'] = data[date_col].dt.floor('H')\n",
                "    bin_counts = data['time_bin'].value_counts()\n",
                "    small_bins = bin_counts[bin_counts < k_time].index\n",
                "    for bin in small_bins:\n",
                "        nearest_bin = bin_counts.idxmax()\n",
                "        data.loc[data['time_bin'] == bin, 'time_bin'] = nearest_bin\n",
                "        bin_counts = data['time_bin'].value_counts()\n",
                "        small_bins = bin_counts[bin_counts < k_time].index\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dp_on_time_bins(data, date_col='date', bin_col='time_bin',\n",
                "                    epsilon_time=EPSILON_TIME,\n",
                "                    sensitivity_hour=SENSITIVITY_HOUR,\n",
                "                    sensitivity_minute=SENSITIVITY_MINUTE):\n",
                "    dp_assignments = {}\n",
                "    for b in data[bin_col].unique():\n",
                "        subset = data[data[bin_col] == b]\n",
                "        avg_h = subset[date_col].dt.hour.mean()\n",
                "        avg_m = subset[date_col].dt.minute.mean()\n",
                "\n",
                "        dp_h = dp_laplace_mechanism(avg_h, epsilon_time, sensitivity_hour)\n",
                "        dp_m = dp_laplace_mechanism(avg_m, epsilon_time, sensitivity_minute)\n",
                "\n",
                "        # round + clamp\n",
                "        dp_h = min(max(int(round(dp_h)), 0), 23)\n",
                "        dp_m = min(max(int(round(dp_m)), 0), 59)\n",
                "\n",
                "        dp_assignments[b] = (dp_h, dp_m)\n",
                "\n",
                "    data['anonymized_date'] = data.apply(lambda row:\n",
                "        row[date_col].replace(\n",
                "            hour=dp_assignments[row[bin_col]][0],\n",
                "            minute=dp_assignments[row[bin_col]][1],\n",
                "            second=0,\n",
                "            microsecond=0\n",
                "        ), axis=1\n",
                "    )\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def main():\n",
                "    # 1) Load data\n",
                "    df = load_data(INPUT_FILE)\n",
                "\n",
                "    # 2) Anonymize user ID\n",
                "    df = anonymize_id(df, id_col='id', date_col='date', salt=SECRET_SALT)\n",
                "\n",
                "    # 3) K-Anonymity for location + DP\n",
                "    df = strict_k_clusters_location(df,\n",
                "                                    lat_col='latitude',\n",
                "                                    lon_col='longitude',\n",
                "                                    k_location=K_LOCATION)\n",
                "    df = dp_on_location_clusters(df,\n",
                "                                 lat_col='latitude',\n",
                "                                 lon_col='longitude',\n",
                "                                 cluster_col='location_cluster',\n",
                "                                 epsilon_loc=EPSILON_LOCATION,\n",
                "                                 sensitivity_lat=SENSITIVITY_LAT,\n",
                "                                 sensitivity_lon=SENSITIVITY_LON)\n",
                "\n",
                "    # 4) K-Anonymity for time + DP\n",
                "    #    First, get time bins ensuring >= K_TIME\n",
                "    df_time = time_binning_k_anonymity(df,\n",
                "                                       date_col='date',\n",
                "                                       k_time=K_TIME)\n",
                "    #    Then, apply DP to each bin\n",
                "    df_time = dp_on_time_bins(df_time,\n",
                "                              date_col='date',\n",
                "                              bin_col='time_bin',\n",
                "                              epsilon_time=EPSILON_TIME,\n",
                "                              sensitivity_hour=SENSITIVITY_HOUR,\n",
                "                              sensitivity_minute=SENSITIVITY_MINUTE)\n",
                "\n",
                "    # 5) Merge time back\n",
                "    #    Now df_time['anonymized_date'] is the new time we want,\n",
                "    #    while df already has the DP location. The two share the same index.\n",
                "    df['anonymized_date'] = df_time['anonymized_date']\n",
                "\n",
                "    # 6) Prepare final columns\n",
                "    anonymized_data = df[['anonymized_id',\n",
                "                          'anonymized_date',\n",
                "                          'anonymized_latitude',\n",
                "                          'anonymized_longitude']].copy()\n",
                "    anonymized_data.columns = ['id', 'date', 'latitude', 'longitude']\n",
                "\n",
                "    # 7) Save to file\n",
                "    anonymized_data.to_csv(OUTPUT_FILE, index=False, sep='\\t', header=False)\n",
                "    print(f\"[INFO] K-Anonymity + DP data saved to: {OUTPUT_FILE}\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
